{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of sample\n",
    "\n",
    "We will create an AI application using next generation search capabilities of Azure AI Search (formerly Azure Cognitive Search). While there are several examples of using AI Search with PaaS database data sources, we will use a SQL Server as our data source. The SQL Servers lives within a private AVS environment. The SQL Server is running on a virtual machine and the connection is exposed through Azure Application Gateway. The SQL server has an SSL connection certificate installed and the connection calls the SQL Server through the domain name listed on that certificate.\n",
    "\n",
    "The end behavior will be something like:\n",
    "\n",
    "```\n",
    "[User search]: Ballons\n",
    "[AI Response]: After searching through our product database, here is what customers are saying: <text>\n",
    "```\n",
    "\n",
    "Behind the scenes, we take the following steps:\n",
    "* Install an SSL certificate on the SQL Server\n",
    "* Set up a sample table in a SQL Server and upload data to it\n",
    "* Set up an index in Azure Cognitive Search to store the data we need, including vectorized versions of the text reviews\n",
    "* Set up an indexer in Azure Cognitive Search to pull data into the index \n",
    "  * Automatically chunks and vectorizes the data using an Azure OpenAI Embedding service\n",
    "* Use Azure Cognitive Search to process the user's query and search for the most relevant data\n",
    "* Use an Azure OpenAI Completion service to respond to the user's query\n",
    "\n",
    "Copyright (c) Microsoft Corporation.\n",
    "Licensed under the MIT license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This sample uses preview features from the [azure-search-documents](https://pypi.org/project/azure-search-documents/#description) package that has not been published on pypi. If you would like to use these preview features, please open a support request on the Search Service resource in the Azure portal, and we will provide instructions.\n",
    "\n",
    "You will also need:\n",
    "* An SSL certificate installed on the SQL Server\n",
    "* SQL Server with server name, DB name, username, and password copied into `example.env`\n",
    "  * The user must have permission to create a new table and enable and view change tracking on the database\n",
    "* An OpenAI resource with the endpoint and key copied into `example.env`\n",
    "* A Azure AI Search resource with the endpoint and key copied into `example.env`\n",
    "* The Python packages listed in `requirements.txt` (can be installed using `pip`)\n",
    "* The Microsoft ODBC 18 driver, [instructions here](https://learn.microsoft.com/en-us/sql/connect/odbc/microsoft-odbc-driver-for-sql-server?view=sql-server-ver16).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "# specify the name of the .env file name \n",
    "env_name = \"example.env\"\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQL Server database connection details\n",
    "server = config[\"server\"]\n",
    "database = config[\"database\"] \n",
    "username = config[\"username\"] \n",
    "password = config[\"password\"] \n",
    "driver = '{ODBC Driver 18 for SQL Server}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Open AI deployment details\n",
    "import openai\n",
    "openai.api_type = config[\"openai_api_type\"]\n",
    "openai.api_key = config['openai_api_key']\n",
    "openai.api_base = config['openai_api_base']\n",
    "openai.api_version = config['openai_api_version'] \n",
    "openai_deployment = config[\"openai_deployment_embedding\"]\n",
    "EMBEDDING_LENGTH = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cognitive Search service details\n",
    "cogsearch_key = config[\"cogsearch_api_key\"]\n",
    "service_endpoint = config[\"cogsearch_endpoint\"]\n",
    "index_name = config[\"cogsearch_index_name\"] # Desired name of index -- does not need to exist already\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to database\n",
    "\n",
    "For simplicity, we set `autocommit=True` in the pyodbc connection parameters, which allows us to execute `ALTER` statements. In a production scenario, setting `autocommit=True` is not recommended; instead, the `ALTER` statements can be executed in SSMS or similar.\n",
    "\n",
    "If a timeout error occurs, retry the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Create a connection string\n",
    "conn_str = f\"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "# Establish a connection to the SQL Server\n",
    "conn = pyodbc.connect(conn_str, autocommit=True)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a database\n",
    "\n",
    "Create the new database, if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{database}') BEGIN CREATE DATABASE {database} END\")\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table in the database\n",
    "\n",
    "We will create a new table \"foodreview\" and upload the data from a csv file. We include a primary key, which is necessary for change tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished dropping table (if existed)\n",
      "Finished creating table\n",
      "Finished creating index\n"
     ]
    }
   ],
   "source": [
    "table_name = \"partysupplies\" \n",
    "\n",
    "# Drop previous table of same name if one exists\n",
    "cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "print(\"Finished dropping table (if existed)\")\n",
    "\n",
    "# Create a table\n",
    "cursor.execute(f\"\"\"\n",
    "               CREATE TABLE {table_name} \n",
    "               (Id int NOT NULL, \n",
    "               CONSTRAINT PK_{table_name}_Id PRIMARY KEY CLUSTERED (Id), \n",
    "               ProductId text, \n",
    "               UserId text, \n",
    "               ProfileName text, \n",
    "               Score integer,\n",
    "               Time bigint,\n",
    "               Summary text, \n",
    "               Text text,\n",
    "               TextConcat text);\n",
    "               \"\"\")\n",
    "print(\"Finished creating table\")\n",
    "\n",
    "# Create a index\n",
    "cursor.execute(f\"CREATE INDEX idx_Id ON {table_name}(Id);\")\n",
    "print(\"Finished creating index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable change tracking\n",
    "\n",
    "This allows us to automatically update the index when changes are made to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('42000', \"[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Change tracking is already enabled for database 'party_supplies'. (5088) (SQLExecDirectW)\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(f\"ALTER DATABASE {database} SET CHANGE_TRACKING = ON (CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON)\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(f\"ALTER TABLE {table_name} ENABLE CHANGE_TRACKING WITH (TRACK_COLUMNS_UPDATED = ON)\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from CSV\n",
    "\n",
    "The data contains a few product reviews, with related info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 8 fields in line 16, saw 9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[286], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_all \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../DataSet/partysupplies.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df_all\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 8 fields in line 16, saw 9\n"
     ]
    }
   ],
   "source": [
    "## Load Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_all = pd.read_csv('../DataSet/partysupplies.csv')\n",
    "\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate data\n",
    "\n",
    "For our example, we will combine the user's summary with the user's review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>TextConcat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>P001</td>\n",
       "      <td>ABG53EBZBKD23</td>\n",
       "      <td>Julie Rogers</td>\n",
       "      <td>5</td>\n",
       "      <td>1333929600</td>\n",
       "      <td>Very happy with the quality</td>\n",
       "      <td>These balloons were perfect for our party. The...</td>\n",
       "      <td>Summary: Very happy with the quality | Review:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>P002</td>\n",
       "      <td>A1FD9E5C06UB6B</td>\n",
       "      <td>Margaret Murphy</td>\n",
       "      <td>3</td>\n",
       "      <td>1301011200</td>\n",
       "      <td>Highly recommend for parties</td>\n",
       "      <td>The streamers added a great touch to our decor...</td>\n",
       "      <td>Summary: Highly recommend for parties | Review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>P003</td>\n",
       "      <td>AOVROBZ8BNTP7</td>\n",
       "      <td>Keith Powers</td>\n",
       "      <td>4</td>\n",
       "      <td>1163376000</td>\n",
       "      <td>Kids had a blast</td>\n",
       "      <td>Napkins were sturdy and absorbed spills well. ...</td>\n",
       "      <td>Summary: Kids had a blast | Review: Napkins we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id ProductId          UserId      ProfileName  Score        Time   \n",
       "0   1      P001   ABG53EBZBKD23     Julie Rogers      5  1333929600  \\\n",
       "1   2      P002  A1FD9E5C06UB6B  Margaret Murphy      3  1301011200   \n",
       "2   3      P003   AOVROBZ8BNTP7     Keith Powers      4  1163376000   \n",
       "\n",
       "                        Summary   \n",
       "0   Very happy with the quality  \\\n",
       "1  Highly recommend for parties   \n",
       "2              Kids had a blast   \n",
       "\n",
       "                                                Text   \n",
       "0  These balloons were perfect for our party. The...  \\\n",
       "1  The streamers added a great touch to our decor...   \n",
       "2  Napkins were sturdy and absorbed spills well. ...   \n",
       "\n",
       "                                          TextConcat  \n",
       "0  Summary: Very happy with the quality | Review:...  \n",
       "1  Summary: Highly recommend for parties | Review...  \n",
       "2  Summary: Kids had a blast | Review: Napkins we...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"TextConcat\"] = df_all.apply(lambda row: f\"Summary: {row['Summary']} | Review: {row['Text']}\",\n",
    "                                    axis = 1)\n",
    "\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into batches\n",
    "batch_size = 30\n",
    "batches = [df_all[i:i + batch_size] for i in range(0, len(df_all), batch_size)]\n",
    "\n",
    "#Iterate over each batch and insert the data into the database\n",
    "for batch in batches:\n",
    "    # Convert the batch dataframe to a list of tuples for bulk insertion\n",
    "    rows = [tuple(row) for row in batch.itertuples(index=False)]\n",
    "    \n",
    "    # Define the SQL query for bulk insertion\n",
    "    query = f\"INSERT INTO {table_name} (Id, ProductId, UserId, ProfileName, Score, Time, Summary, Text, TextConcat) \\\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    cursor.executemany(query, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example query\n",
    "\n",
    "This checks that the data was uploaded correctly. We should have 99 rows at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(f\"SELECT count(Id) FROM {table_name};\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data source connection in Azure AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed AI Search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.indexes.models import SemanticSearch\n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryLanguage,\n",
    "    QueryType,\n",
    "    VectorizedQuery,\n",
    "    VectorQuery,\n",
    "    VectorizableTextQuery,\n",
    "    VectorFilterMode,    \n",
    ")\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    AzureOpenAIEmbeddingSkill,  \n",
    "    AzureOpenAIParameters,  \n",
    "    AzureOpenAIVectorizer,  \n",
    "    ExhaustiveKnnParameters,  \n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    FieldMapping,  \n",
    "    HnswParameters,  \n",
    "    HnswAlgorithmConfiguration,\n",
    "    IndexProjectionMode,  \n",
    "    InputFieldMappingEntry,\n",
    "    MergeSkill,\n",
    "    OutputFieldMappingEntry,  \n",
    "    SemanticPrioritizedFields,    \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SearchIndex,  \n",
    "    SearchIndexer,  \n",
    "    SearchIndexerDataContainer,  \n",
    "    SearchIndexerDataSourceConnection,  \n",
    "    SearchIndexerIndexProjectionSelector,  \n",
    "    SearchIndexerIndexProjections,  \n",
    "    SearchIndexerIndexProjectionsParameters,  \n",
    "    SearchIndexerSkillset,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,  \n",
    "    SemanticSearch,  \n",
    "    SplitSkill,  \n",
    "    SqlIntegratedChangeTrackingPolicy,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    VectorSearchAlgorithmKind,  \n",
    "    VectorSearchAlgorithmMetric,  \n",
    "    VectorSearchProfile,  \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data source connection\n",
    "\n",
    "This step creates a connection that will be used to pull data from our SQL table.\n",
    "\n",
    "Documentation can be found [here.](https://learn.microsoft.com/en-us/azure/search/search-howto-connecting-azure-sql-database-to-azure-search-using-indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'sqlserver-index-test-sqlserver-connection' created or updated\n"
     ]
    }
   ],
   "source": [
    "ds_conn_str = f'Encrypt=True;TrustServerCertificate=True;Connection Timeout=30;Server=tcp:{server};Database={database};User ID={username};Password={password};'\n",
    "\n",
    "cogsearch_credential = AzureKeyCredential(cogsearch_key)\n",
    "ds_client = SearchIndexerClient(service_endpoint, cogsearch_credential)\n",
    "container = SearchIndexerDataContainer(name=table_name)\n",
    "\n",
    "change_detection_policy = SqlIntegratedChangeTrackingPolicy()\n",
    "\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-sqlserver-connection\",\n",
    "    type=\"azuresql\",\n",
    "    connection_string=ds_conn_str,\n",
    "    container=container,\n",
    "    data_change_detection_policy=change_detection_policy\n",
    ")\n",
    "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up automatic chunking + vectorization + indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index\n",
    "\n",
    "The plan is:\n",
    "1. Take the combined text (summary + review text) from each product review\n",
    "2. Split the combined text into chunks\n",
    "3. Embed each chunk as a vector\n",
    "4. (Later) search for the most relevant chunk based on the incoming query. \n",
    "\n",
    "To enable this, the search index will store all of the following data, for each chunk of text:\n",
    "* Id of chunk\n",
    "* Chunk text\n",
    "* Vector version of chunk text\n",
    "* Id of parent row\n",
    "* Product Id from parent row\n",
    "* Review text from parent row\n",
    "* Summary text from parent row\n",
    "* Score from parent row\n",
    "\n",
    "All of these values will be stored in SearchFields specified in the code below.\n",
    "\n",
    "In this step we also configure the search algorithm(s), and the vectorizer that will automatically vectorize the incoming query.\n",
    "\n",
    "Documentation about creating indexes can be found [here.](https://learn.microsoft.com/en-us/azure/search/search-how-to-create-search-index?tabs=index-other-sdks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "semantic_settings is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchIndex'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlserver-index-test created\n"
     ]
    }
   ],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=cogsearch_credential)\n",
    "\n",
    "fields = [\n",
    "    # Properties of individual chunk\n",
    "    SearchField(name=\"Id\", type=SearchFieldDataType.String, key=True,\n",
    "                sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                vector_search_dimensions=EMBEDDING_LENGTH, vector_search_profile_name=\"my-vector-search-profile\"),\n",
    "    # Properties of original row in DB that the chunk belonged to\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_product_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_text\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_summary\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"parent_score\", type=SearchFieldDataType.Int64, sortable=True, filterable=True, facetable=True)\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"my-hnsw-config\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"my-vector-search-profile\",\n",
    "            algorithm_configuration_name=\"my-hnsw-config\",\n",
    "            vectorizer=\"my-openai\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            name=\"my-openai\",\n",
    "            kind=\"azureOpenAI\",\n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "                resource_uri=openai.api_base,\n",
    "                deployment_id=openai_deployment,\n",
    "                api_key=openai.api_key\n",
    "            )\n",
    "        )  \n",
    "    ]  \n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        content_fields=[SemanticField(field_name=\"Id\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'{result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create skillset\n",
    "\n",
    "We use two pre-built skills:\n",
    "1. The Split Skill takes the concatenated text and divides it into chunks (to stay within the token limits for the OpenAI embedding service).\n",
    "2. The Azure Open AI Embedding Skill takes the outputs of the Split Skill and vectorizes them individually.\n",
    "\n",
    "Afterwards, we apply an Index Projector to make it so that our final index has one item for every chunk of text (rather than one item for every original row in the DB).\n",
    "\n",
    "We recommend the following resources to learn more about the process and how one can adapt it to different applications:\n",
    "* [Overview of indexers](https://learn.microsoft.com/en-us/azure/search/search-indexer-overview)\n",
    "* [Skill context and input annotation language](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-annotation-language)\n",
    "* [Reference inputs and outputs in skillsets](https://learn.microsoft.com/en-us/azure/search/cognitive-search-concept-annotations-syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sqlserver-index-test-skillset created\n"
     ]
    }
   ],
   "source": [
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\"\n",
    "\n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=300,  \n",
    "    page_overlap_length=20,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/TextConcat\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ]  \n",
    ")\n",
    "\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_uri=openai.api_base,  \n",
    "    deployment_id=openai_deployment,  \n",
    "    api_key=openai.api_key,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "    ]  \n",
    ")  \n",
    "\n",
    "index_projections = SearchIndexerIndexProjections(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=index_name,  \n",
    "            parent_key_field_name=\"parent_id\", # Note: this populates the \"parent_id\" search field\n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
    "                InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\"),\n",
    "                InputFieldMappingEntry(name=\"parent_product_id\", source=\"/document/ProductId\"),\n",
    "                InputFieldMappingEntry(name=\"parent_text\", source=\"/document/Text\"),\n",
    "                InputFieldMappingEntry(name=\"parent_summary\", source=\"/document/Summary\"),\n",
    "                InputFieldMappingEntry(name=\"parent_score\", source=\"/document/Score\")\n",
    "            ],  \n",
    "        ),  \n",
    "    ],\n",
    ")  \n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=[split_skill, embedding_skill],\n",
    "    index_projections=index_projections  \n",
    ")\n",
    "  \n",
    "client = SearchIndexerClient(service_endpoint, cogsearch_credential)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f' {skillset.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sqlserver-index-test-indexer created\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to chunk documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name\n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(service_endpoint, cogsearch_credential)\n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "\n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)\n",
    "print(f' {indexer_name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer status: running\n"
     ]
    }
   ],
   "source": [
    "# Get the status of the indexer  \n",
    "indexer_status = indexer_client.get_indexer_status(indexer_name)\n",
    "print(f\"Indexer status: {indexer_status.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow some time for the indexer to process the data\n",
    "import time\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use vector search for sample application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"ballons\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following output, we find the top 3 chunks that are most relevant to the user's query.\n",
    "\n",
    "Feel free to retry the following cell in case of an empty response or a 429 error. An empty response probably indicates that the chunking/embedding process has not finished yet. A 429 error means there have been too many requests to the OpenAI embedding service and should go away on retrying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search score: 0.872132\n",
      "Parent Id: 186 | Chunk id: bc8f505cc50b_186_pages_0\n",
      "Product Id: P001\n",
      "Text chunk: Summary: Fun and festive | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\n",
      "Review summary: Fun and festive\n",
      "Review text: These balloons were perfect for our party. They lasted all day and were very colorful.\n",
      "Review score: 5\n",
      "-----\n",
      "Search score: 0.8716175\n",
      "Parent Id: 206 | Chunk id: bc8f505cc50b_206_pages_0\n",
      "Product Id: P001\n",
      "Text chunk: Summary: Lovely and charming | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\n",
      "Review summary: Lovely and charming\n",
      "Review text: These balloons were perfect for our party. They lasted all day and were very colorful.\n",
      "Review score: 5\n",
      "-----\n",
      "Search score: 0.8716175\n",
      "Parent Id: 146 | Chunk id: bc8f505cc50b_146_pages_0\n",
      "Product Id: P001\n",
      "Text chunk: Summary: Lovely and charming | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\n",
      "Review summary: Lovely and charming\n",
      "Review text: These balloons were perfect for our party. They lasted all day and were very colorful.\n",
      "Review score: 5\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(service_endpoint, index_name, credential=cogsearch_credential)\n",
    "vector_query = VectorizableTextQuery(text=user_query, k_nearest_neighbors=3, fields=\"vector\", exhaustive=True)\n",
    "# Use the query below to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(user_query), k=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(\n",
    "    search_text=None,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"Id\", \"parent_id\", \"chunk\", \"parent_product_id\", \"parent_text\", \"parent_summary\", \"parent_score\"],\n",
    "    top=3\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Search score: {result['@search.score']}\")\n",
    "    print(f\"Parent Id: {result['parent_id']} | Chunk id: {result['Id']}\")\n",
    "    print(f\"Product Id: {result['parent_product_id']}\")\n",
    "    print(f\"Text chunk: {result['chunk']}\") \n",
    "    print(f\"Review summary: {result['parent_summary']}\")\n",
    "    print(f\"Review text: {result['parent_text']}\")\n",
    "    print(f\"Review score: {result['parent_score']}\")\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate GPT Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template \n",
    "template = \"\"\"\n",
    "    The user's query is: {query}\n",
    "    The most relevant product review is: {context}\n",
    "    The user is searching for a product matching their query. \n",
    "    Tell the user that after searching through our product database, you provide the reviews associated with the product. \n",
    "    Your answer should summarize the review text,\n",
    "    include the product ID, and mention the score given in the review.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product id: P001. Review text: These balloons were perfect for our party. They lasted all day and were very colorful.. Review score: 5Product id: P001. Review text: These balloons were perfect for our party. They lasted all day and were very colorful.. Review score: 5Product id: P001. Review text: These balloons were perfect for our party. They lasted all day and were very colorful.. Review score: 5\n"
     ]
    }
   ],
   "source": [
    "# create the context from the search response (requires regenerating results)\n",
    "results = search_client.search(\n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"Id\", \"chunk\", \"parent_product_id\", \"parent_text\", \"parent_score\"],\n",
    "    top=3\n",
    ")\n",
    "\n",
    "context = \"\"\n",
    "for result in results:\n",
    "    context += f\"Product id: {result['parent_product_id']}. Review text: {result['parent_text']}. Review score: {result['parent_score']}\"\n",
    "    \n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The user's query is: ballons\n",
      "    The most relevant product review is: Product id: P001. Review text: These balloons were perfect for our party. They lasted all day and were very colorful.. Review score: 5Product id: P001. Review text: These balloons were perfect for our party. They lasted all day and were very colorful.. Review score: 5Product id: P001. Review text: These balloons were perfect for our party. They lasted all day and were very colorful.. Review score: 5\n",
      "    The user is searching for a product matching their query. \n",
      "    Tell the user that after searching through our product database, you provide the reviews associated with the product. \n",
      "    Your answer should summarize the review text,\n",
      "    include the product ID, and mention the score given in the review.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(context=context, query=user_query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"b72983ed-ace7-4cc0-8d81-8220c66ee774\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The requested information is not available in the retrieved data. Please try another query or topic.\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null,\n",
      "        \"end_turn\": true,\n",
      "        \"context\": {\n",
      "          \"citations\": [\n",
      "            {\n",
      "              \"content\": \"Summary: Great for all kinds of parties | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\\n151\\nP001\\nGreat for all kinds of parties\\nThese balloons were perfect for our party. They lasted all day and were very colorful.\",\n",
      "              \"title\": null,\n",
      "              \"url\": null,\n",
      "              \"filepath\": null,\n",
      "              \"chunk_id\": \"0\"\n",
      "            },\n",
      "            {\n",
      "              \"content\": \"Summary: Great for all kinds of parties | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\\n216\\nP001\\nGreat for all kinds of parties\\nThese balloons were perfect for our party. They lasted all day and were very colorful.\",\n",
      "              \"title\": null,\n",
      "              \"url\": null,\n",
      "              \"filepath\": null,\n",
      "              \"chunk_id\": \"0\"\n",
      "            },\n",
      "            {\n",
      "              \"content\": \"Summary: Made our event memorable | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\\n41\\nP001\\nMade our event memorable\\nThese balloons were perfect for our party. They lasted all day and were very colorful.\",\n",
      "              \"title\": null,\n",
      "              \"url\": null,\n",
      "              \"filepath\": null,\n",
      "              \"chunk_id\": \"0\"\n",
      "            },\n",
      "            {\n",
      "              \"content\": \"Summary: Made our event amazing | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\\n76\\nP001\\nMade our event amazing\\nThese balloons were perfect for our party. They lasted all day and were very colorful.\",\n",
      "              \"title\": null,\n",
      "              \"url\": null,\n",
      "              \"filepath\": null,\n",
      "              \"chunk_id\": \"0\"\n",
      "            },\n",
      "            {\n",
      "              \"content\": \"Summary: Made our event memorable | Review: These balloons were perfect for our party. They lasted all day and were very colorful.\\n266\\nP001\\nMade our event memorable\\nThese balloons were perfect for our party. They lasted all day and were very colorful.\",\n",
      "              \"title\": null,\n",
      "              \"url\": null,\n",
      "              \"filepath\": null,\n",
      "              \"chunk_id\": \"0\"\n",
      "            }\n",
      "          ],\n",
      "          \"intent\": \"[\\\"What are balloons made of?\\\", \\\"How are balloons made?\\\", \\\"Different types of balloons\\\", \\\"Uses of balloons\\\", \\\"History of balloons\\\"]\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1720547705,\n",
      "  \"model\": \"gpt-35-turbo-16k\",\n",
      "  \"object\": \"extensions.chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 47,\n",
      "    \"prompt_tokens\": 3469,\n",
      "    \"total_tokens\": 3516\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "client = openai.AzureOpenAI(\n",
    "    azure_endpoint=config[\"openai_api_base\"],\n",
    "    api_key=config[\"openai_api_key\"],\n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-35-turbo-16k\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_query,\n",
    "        },\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"data_sources\":[\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": config[\"cogsearch_endpoint\"],\n",
    "                    \"index_name\": config[\"cogsearch_index_name\"],\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": config[\"cogsearch_api_key\"],\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "After finishing the sample, remember to delete unneeded resources:\n",
    "* Table created within existing SQL SERVER DB\n",
    "* Within the Search Service resource:\n",
    "  * Data source connection\n",
    "  * Index\n",
    "  * Skillset\n",
    "  * Indexer\n",
    "\n",
    "These can always be recreated by rerunning the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
